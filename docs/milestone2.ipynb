{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2tYJmdaj1iiC"
   },
   "source": [
    "# <center> CS207 Final Project: Milestone 2 - November 19 2019 </center>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "<center> Geng Yichen, Jian Yingsi, Meeus Matthieu, Zhang Lihong </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZKmny9UOYz2W"
   },
   "source": [
    "# 1 Introduction\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_cBVisQoENuI"
   },
   "source": [
    "Derivatives come up in every aspect of science and engineering. Calculus taught us how to derive analytical expressions of functional derivatives, but in many cases this is either impossible or too much of a hassle. Therefore, numerical methods or algorithmic approaches to compute the derivative are extremely important. \n",
    "Methods of computing functions' derivatives in computer programs can be classified into 4 types: \n",
    "\n",
    "(1) Determining derivatives by hands and coding them. <br/>\n",
    "(2) Symbolic differentiation in computer tools, such as Mathematica and Maple. <br/>\n",
    "(3) Numerical methods: using finite differences to approxiate derivatives. <br/>\n",
    "(4) Automatic Differentiation, which is the subject of our project. <br/>\n",
    "\n",
    "Automatic differentiation (AD) is a set of techniques for evaluating functional derivatives efficiently and accurately in computer programs. For any analytic function f(x) to be differentiated at a point $x_0$, AD first rewrites f(x) as a combination of elementary functions, then determining the derivative values of f(x) through combining derivatives of elementary functions by the chain rule. Since the derivative values of all elementary functions are known and accurate, and the procedures of AD have no potential sources of errors except tiny rounding errors due to machine precision, thus the derivative values obtained by AD are accurate. As for other differentiation methods, manual calculating functional derivatives and coding them by hands can be tedious, time-consuming and prone to make mistakes; symbolic differentiation could return rigmarole and unreadable symbolic expressions; and numerical method of finite differences could be ill-conditioned due to truncation and round-off errors, and is also inappropriate to handle functional derivatives with many independent variables. \n",
    "\n",
    "For example, in the numerical method of finite difference, the derivative is calculated as the following where the limit for h is approached but not put to zero:\n",
    "\n",
    "$$\\frac{df}{dx} \\approx \\frac{f(x+h) - f(x)}{h}$$\n",
    "\n",
    "While this approach yields decent results in many cases, it is never completely accurate. For too large values of h, the error originates from the intrinsic error of the finite difference derivative. For too small values of h, the error originates from rounding errors. \n",
    "\n",
    "Although AD does not show explicit derivative expressions, people usually only want to obtain derivative values at some points rather than symbolic expressions, thus AD is much better than other differentiation methods with respect to determining functional derivative values.\n",
    "\n",
    "In modern differentiation computation, AD is the most efficient and accurate tool to implement differentiation in computer programs. In this project, we will design an accurate, user-oriented differentiation calculator by implementing AD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F-0jo3nlx-_G"
   },
   "source": [
    "# 2 Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JwWhNMOfyFh1"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "Automatic differentiation is an algorithmic approach to compute derivatives up until machine precision accuracy. It has two common methods: forward and reverse mode. In this project, only the forward mode will be discussed at first. Forward automatic differentiation leverages the chain rule to split the differentiation of a complex function in elementary functions and makes use of the easy derivatives of elementary functions.  If $h(u(t))$ is the function of which the derivative is required, the chain rule for partial derivatives says:\n",
    "\n",
    "$$\\frac{\\partial h}{\\partial t} = \\frac{\\partial h}{\\partial u}*\\frac{\\partial u}{\\partial t}$$\n",
    "\n",
    "Or for $h(u(t), v(t))$:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial h}{\\partial t} = \\frac{\\partial h}{\\partial u}*\\frac{\\partial u}{\\partial t} + \\frac{\\partial h}{\\partial v}*\\frac{\\partial v}{\\partial t}\n",
    "\\end{equation}\n",
    "\n",
    "Hence, the computation of the derivatives of complicated functions that consist of multiple, consequent elementary operations can be split into the multiplication and addition of derivatives of the elementary functions in the following table. Examples of the elementary operations are addition and multiplication but also sine, cosine and log. The complete list of elementary functions that will be incorporated in this package can be found in 'Implementation'. A subset is illustrated in the table below:\n",
    "\n",
    "| Elementary function        | Example | \n",
    "| :-------------: |:-------------:|\n",
    "| Field Operations    | $+ - * / $ |\n",
    "| Powers     |$x^2, x^6$|\n",
    "| Roots     |$\\sqrt{x}$|\n",
    "| Trigonometric     |$sin(x)$|\n",
    "| Inverse Trigonometric     |$asin(x)$|\n",
    "| Logarithmic     |$log(x)$|\n",
    "| Exponential    |$e^x$|\n",
    "\n",
    "The split of the complex function into its elementary operations is commonly visualized in a so-called computational graph. This summarizes the sequence of operations that need to be done for the evaluation and differentiation of the function. An example for the simple function $f(x,y) = xy + exp(xy) $ is given in the image below. Note that this example comes from Lecture 12.\n",
    "\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/BackPropagators/cs207-FinalProject/master/docs/Images/CompGraph.png \"Computational Graph\")\n",
    "\n",
    "\n",
    "The actual implementation of the forward mode can be better understood using the so-called seed vectors $p_i$ for every variable $x_i$. This means that, every time the derivative of a function is computed, the derivative is 'seeded', or multiplied (dot product) by the corresponding seed. An example might clarify this. Imagine function $f = x*cos(y)$, where x and y are the variables corresponding to seed vectors $p_1 = [1,0]$ and $p_2 = [0,1]$ respectively. The directional derivative $D_p$ of f will then be computed as follows:\n",
    "\n",
    "$$D_px_3 = \\sum_{i}^n \\frac{\\partial x_3}{x_i} p_i$$\n",
    "\n",
    "Where $x_1$, $x_2$ and $x_3$ correspond to x, y and $x*cos(y)$. We can then specify the direction p to find the desired derivative. For instance, for $p = [1,0]$, we recover:\n",
    "\n",
    "$$D_px_3 = \\frac{\\partial x_3}{\\partial x_1} p_1 + \\frac{\\partial x_3}{\\partial x_2}p_2 = \\frac{\\partial x_3}{\\partial x_1} = \\frac{\\partial f}{\\partial x}$$\n",
    "\n",
    "Or mathematically:\n",
    "\n",
    "$$D_px_3 = \\nabla x_3 \\cdot p$$\n",
    "\n",
    "This means that the forward mode of automatic differentiation is actually computing the dot product of the gradient and the seed vector. Choosing the seed vectors appropriately, either this product or the full gradient can be recovered, dependent on the application.\n",
    "\n",
    "For functions in higher dimensions h(x): ${\\rm I\\!R}^m \\rightarrow {\\rm I\\!R}^n$, the entire Jacobian will be computed using the same, simple approach. Recall the definition of the Jacobian J in ${\\rm I\\!R}^{nxm}$ for function h(x):\n",
    "\n",
    "$$\\mathbf{J}=\\left[\\begin{array}{cccc}\n",
    "\\frac{\\partial h_1}{\\partial x_1} & \\frac{\\partial h_1}{\\partial x_2} & .. & \\frac{\\partial h_1}{\\partial x_m} \\\\\n",
    "\\frac{\\partial h_2}{\\partial x_1} & \\frac{\\partial h_2}{\\partial x_2} & .. & \\frac{\\partial h_2}{\\partial x_m} \\\\\n",
    ".. & .. & .. & .. \\\\\n",
    "\\frac{\\partial h_n}{\\partial x_1} & \\frac{\\partial h_n}{\\partial x_2} & .. & \\frac{\\partial h_n}{\\partial x_m}\n",
    "\\end{array}\\right]$$\n",
    "\n",
    "Note that the seed vector approach is still valid for this high-dimensional case. So teh forward mode is actually computing the product of the Jacobian with the seed vector. Again, the entire Jacobian can be recoverd by choosing the seed vectors appropriately and adding the resulting products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DeyiZBf-4AHU"
   },
   "source": [
    "# 3 How to Use AutoDiff\n",
    "\n",
    "## 3.1 How to install AutoDiff\n",
    "There are two methods of installing our package: Github and PyPI. The installation details are listed below. After installation, the user should import the class Var, which represents variables, from our package (shown as the pseudocode below) and other dependencies (such as numpy and math) since we import those dependencies to our package modules. With the imported package, the user can define x as an object of Var, and give the initial input of x. Then the user can apply function f on x, which returns f(x) with new values and jacobian. \n",
    "### 3.1.1 Installation from Github\n",
    "\n",
    "\n",
    "*   Step 1: Download our package from Github to users' local directory by the following commands in the terminal\n",
    "```\n",
    "mkdir AutoDiff\n",
    "cd AutoDiff\n",
    "git clone https://github.com/BackPropagators/cs207-FinalProject.git\n",
    "cd cs207-FinalProject\n",
    "```\n",
    "\n",
    "*   Step 2: Create and Activate a virtual environment in conda.\n",
    " * If the user don't have conda, she/he should install it. Then the user can create his/her own virtual environment in conda by the following codes, where yourname is the name the user wants to call the newly created environment.\n",
    "```\n",
    "conda create -n yourname python=3.6 anaconda\n",
    "```\n",
    " * Activate the user's virtual environment\n",
    " ```\n",
    " source activate yourname\n",
    " ```\n",
    " * Install the required dependencies and run the given tests in AutoDiff\n",
    " ```\n",
    " pip install -r requirements.txt\n",
    " pytest\n",
    " ```\n",
    " If pytest is not installed, then the user can install it by\n",
    " ```\n",
    " pytest install -U pytest\n",
    " ```\n",
    "\n",
    " * Use AutoDiff Python package in the terminal (See demo in 3.2)\n",
    " ```python\n",
    ">>> from AutoDiff.ForwardAD import Var\n",
    ">>> x = Var(1)\n",
    ">>> f = x + 1\n",
    ">>> f.get_value()\n",
    "2\n",
    ">>> f.get_jacobian()\n",
    "[1]\n",
    "...\n",
    ">>> quit()\n",
    "```\n",
    "```\n",
    "# deactivate virtual environment\n",
    "conda deactivate\n",
    "```\n",
    "\n",
    "\n",
    "### 3.1.2 Installation from PyPI.\n",
    "We will use PyPI to distribute our package in our futre development of the package.\n",
    "\n",
    "\n",
    "## 3.2 Basic Demo \n",
    "Install our package, and import the module and dependencies \n",
    "```python\n",
    "pip install AutoDiff\n",
    "python\n",
    ">>> from AutoDiff.ForwardAd import Var\n",
    ">>> import numpy as np\n",
    "```\n",
    "instantiate a `Var` class.\n",
    "```\n",
    ">>> x = Var(1.0)\n",
    "```\n",
    "Define a function.\n",
    "```python\n",
    ">>> f = x + 1\n",
    "```\n",
    "Evaluate the function value which is expected to be 2.0.\n",
    "```python\n",
    ">>> f.get_value()\n",
    "2.0\n",
    "```\n",
    "Evaluate the function jacobian which is exepcted to be [1.0].\n",
    "```python\n",
    ">>> f.get_jacobian()\n",
    "[1.0]\n",
    "```\n",
    "\n",
    "Evaluate the value and jacobian of scaler function with one variable.\n",
    "```python\n",
    "# basic operation\n",
    "# Expect the value of f is 3.0, and the derivative of f is [2.0]\n",
    ">>> x = Var(1)\n",
    ">>> f = 2*x + 1\n",
    ">>> f.get_value()\n",
    "3.0\n",
    ">>> f.jacobian\n",
    "[2.0]\n",
    "\n",
    "# exponential function\n",
    "# Expect the value of f is around 2.718281828459, and the derivative is around [2.718281828459]\n",
    ">>> f = Var.exp(x)\n",
    ">>> f.get_value()\n",
    "2.718281828459045\n",
    ">>> f.get_jacobian()\n",
    "[2.718281828459045]\n",
    "\n",
    "# logarithmic function\n",
    "# Expect the value of f is 0.0, and the derivative of f is around [0.43429448190325]\n",
    ">>> f = Var.log(x, b=10)\n",
    ">>> f.get_value()\n",
    "0.0\n",
    ">>> f.get_jacobian()\n",
    "[0.43429448190325176]\n",
    "\n",
    "# trignometric function\n",
    "# Expect the value of f is 0.49999999999999994, and the derivative is [0.8660254037844]\n",
    ">>> y = Var(np.pi/6)\n",
    ">>> f = Var.sin(y)\n",
    ">>> f.get_value()\n",
    "0.49999999999999994\n",
    ">>> f.get_jacobian()\n",
    "[0.8660254037844387]\n",
    "```\n",
    "\n",
    "# 4 Software Organization\n",
    "\n",
    "\n",
    "**4.1 Project Directory Structure**\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "                    CS207-FinalProject/\n",
    "                                       README.md\n",
    "                                       LICENSE\n",
    "                                       setup.py\n",
    "                                       AutoDiff/\n",
    "                                                _init_.py\n",
    "                                                ForwardAD.py\n",
    "                                       docs/\n",
    "                                                     milestone1.ipynb\n",
    "                                                     milestone2.ipynb\n",
    "                                       test/\n",
    "                                            test_all.py\n",
    "                                       demo/\n",
    "                                            presentation.pdf\n",
    "                                       ...\n",
    "                   \n",
    "                           \n",
    "**4.2 Included Modules and Basic Functionality**\n",
    "\n",
    "AutoDiff: This module includes implementation for classes AutoDiff and Multifunc. AutoDiff and Multifunc implement the forward mode of auto differentiation. AutoDiff takes care of scalar function inputs (with single and multiple variables) and Multifunction takes care of vector function inputs. \n",
    "\n",
    "Test: This module includes all the test suites for the package.\n",
    "\n",
    "\n",
    "**4.3 Test**\n",
    "\n",
    "The test suites live in *CS207-FinalProject/Test/* folder.  We will use Travis CI to run tests and use CodeCov to automatically checks code coverage. \n",
    "\n",
    "\n",
    "**4.4 Distributing Our Package**\n",
    "\n",
    "We will use PyPI to distribute our package. The Python Package Index (PyPI) is a repository of software for the Python programming language.\n",
    "\n",
    "\n",
    "**4.5 Packaging**\n",
    "\n",
    "We will follow the Python Packaging User Guide published by the Python Packaging Authority(PyPA), which is the authoritative resource on how to package, publish, and install Python projects using current tools. We will use Setuptools because it is comprehensive, actively-maintained, and stable to  create an package for easy distibution out of our project.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0CDPTBgw1_WT"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ypeCT0O3UHqw"
   },
   "source": [
    "# 5 Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l5tzv61IUOXV"
   },
   "source": [
    "**5.1 Core Data Structures**\n",
    "The core data structure for storing the jacobian is python dictionary, where the key is the variable (an instance of `Var`, see **Core Classes** below) and the value is its partial derivative.\n",
    "\n",
    "**5.2 Core Classes**\n",
    "In the `ForwardAD.py`, we implemented the `Var` class, which defines the variables in a function and calculates the jacobian of scalar function with single and multiple variables (multivariables will be tested in the future; see 5.6 for details). We have also implemented the `MultiFunc` class, which calculates the jacobian of vector functions with single and multiple variables (`MultiFunc` will be tested in the future, so it is not included in the code; see 5.6 for details). \n",
    "\n",
    "**5.3 Important Attributes**\n",
    "The `Var` class has two private attributes: `self._val` and `self._jacobian`. `self._val` stores the value of the current function, and `self._jacobian` is a dictionary whose key is a `Var` instance and value is its partial derivative. `self._jacobian` is initialized as {self: 1} in the constructor. When there is only one variable in the function, after each operation, only the value (jacobian) may be updated whereas the key (variable) remains the same in the new dictionary. When there are multiple variables in the function, after each operation, values (jacobian) may be updated and new key (variable) may be added in the new dictionary.\n",
    "\n",
    "**5.4 External Dependencies**\n",
    "We will rely on\n",
    "- Numpy: a Python library supporting large, multi-dimensional arrays and matrices, with a large collection of high-level mathematical functions.\n",
    "- math: a Python library containing a great many mathematical operations.\n",
    "- pytest: a framework supporting complext tests and make small tests easy.\n",
    "\n",
    "\n",
    "**5.5 Elementary Functions**\n",
    "We defined methods that deal with these elementary functions inside the `Var` class. To use these functions, simply call `Var.function_name`. See demo in 3.2.\n",
    "\n",
    "**5.6 Multivariable Function and Vector Functions**\n",
    "To get the jacobian of a multivariable function, we ask to user to supply the order of `Var` in a list. The following code demoes the process.\n",
    "```python\n",
    "from AutoDiff.ForwardAD import Var\n",
    ">>> x = Var(1)\n",
    ">>> y = Var(2)\n",
    ">>> f = x + 2*y\n",
    ">>> f.get_jacobian_([x, y])\n",
    "[1.0, 2.0]\n",
    "```\n",
    "\n",
    "For vector functions, we ask to user to supply the functions in a list. The following code demoes the process.\n",
    "```python\n",
    "from AutoDiff.ForwardAD import MultiFunc\n",
    ">>> x = Var(1)\n",
    ">>> F = MultiFunc([x+1, 2*x])\n",
    ">>> F.get_jacobian()\n",
    "[[1], [2]]\n",
    "```\n",
    "\n",
    "**5.6 Methods in Var**\n",
    "\n",
    "```python\n",
    "class Var:\n",
    "    def __init__(self, a, j):\n",
    "        self.val = a\n",
    "        self.jacobian = j\n",
    "    \n",
    "    def abs(self):\n",
    "      pass\n",
    "\n",
    "    def neg(self):\n",
    "      pass\n",
    "\n",
    "    def __add__(self, other):\n",
    "      pass\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "      pass\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "      pass\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "      pass\n",
    "\n",
    "    def __mul__(self, other):\n",
    "      pass\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "      pass\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "      pass\n",
    "    \n",
    "    def __rtruediv__(self, other):\n",
    "      pass\n",
    "\n",
    "    def __pow__(self, other):\n",
    "      pass\n",
    "    \n",
    "    def __rpow__(self, other):\n",
    "      pass\n",
    "\n",
    "    def exp(self):\n",
    "      pass\n",
    "    \n",
    "    def log(self):\n",
    "      pass\n",
    "    \n",
    "    def sqrt(self):\n",
    "      pass\n",
    "    \n",
    "    def sin(self):\n",
    "      pass\n",
    "    \n",
    "    def cos(self):\n",
    "      pass\n",
    "\n",
    "    def tan(self):\n",
    "      pass\n",
    "\n",
    "    def arctan(self):\n",
    "      pass\n",
    "\n",
    "    def arcsin(self):\n",
    "      pass\n",
    "\n",
    "    def arccos(self):\n",
    "      pass\n",
    "\n",
    "    def sinh(self):\n",
    "      pass\n",
    "\n",
    "    def cosh(self):\n",
    "      pass\n",
    "\n",
    "    def tanh(self):\n",
    "      pass\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VHGmIuzBV3aF"
   },
   "source": [
    "# 6 Future Features\n",
    "\n",
    "For the moment, only single functions of single variables have been included and tested in the submitted code. However, this has been thought through as discussed in the implementation and will be included for the final deliverables. \n",
    "\n",
    "With the forward mode of automatic differentiation up and running, additional features that rely on differentiation up until machine precision can be developed. This section will discuss the mathematical background of root-finding and optimization, which will be implemented as additional features of the packages for the final version.\n",
    "\n",
    "**6.1 Root-finding**\n",
    "\n",
    "First, root-finding arises in many applications in any kind of engineering and science. Consider the general problem in which the purpose is to find that x-value that satisfies the following equation for a certain function f:\n",
    "\n",
    "$$f(x) = 0$$\n",
    "\n",
    "There are multiple numerical approaches to iteratively find this x-value, including the famous Newton's method. Here, a certain starting value $x^{(0)}$ is chosen, after which the following equation is used to update $x^{(k)}$ until a particular convergence criterium is reached:\n",
    "\n",
    "$$x^{(k)} = x^{(k)}-{\\frac {f(x^{(k)})}{f'(x^{(k)})}}\\$$\n",
    "\n",
    "Note that the derivative of f with respect to x evaluated at $x^{(k)}$ plays a crucial role in this equation. In many cases, analytical expressions for $f'(x)$ are not available and numerical differentiation methods are needed. With the forward mode of the automatic differentiation working, the Newton's method for root-finding can be implemented. It is foreseen that the the user interacts as follows with this module of the package AutoDiff:\n",
    "\n",
    "```python\n",
    ">>> from AutoDiff.ForwardAd import Var\n",
    ">>> from AutoDiff import Newton\n",
    ">>> def f(x):\n",
    "    return Var.sin(x)**(Var.sqrt(x))\n",
    ">>> x_0 = 0.1\n",
    ">>> root = Newton(f, x_0)\n",
    "```\n",
    "\n",
    "In multiple dimensions the iterative expression becomes:\n",
    "\n",
    "$$\\mathbf {x} ^{(k+1)}=\\mathbf {x} ^{(k)}-[\\mathbf {H} f(\\mathbf {x} ^{(k)})]^{-1}\\nabla f(\\mathbf {x} ^{(k)}) $$\n",
    "\n",
    "Here, $\\mathbf {x}$ represents a vector and $\\mathbf {H}$ and $\\nabla f$ the Hessian and gradient of f with respect to $\\mathbf {x}$ respectively.\n",
    "\n",
    "\n",
    "**6.2 Optimization**\n",
    "\n",
    "A second, highly relevant application of numerical differentiation can be in optimization. In many problems, and most notably in Computer Science and Machine Learning in the last years, is it crucial to efficiently minimize a certain function F in terms of multiple variables $\\mathbf {x} = (x_1, ..., x_n)$. Or mathematically:\n",
    "\n",
    "$$\\mathbf {x}_{solution} = \\underset{\\mathbf {x}}{argmin}   F(\\mathbf {x})$$\n",
    "\n",
    "A common numerical method that iteratively computes the solution for this equation is gradient descent. In this algorithm, an initial guess is iteratively updated by the taking a step in the direction that is alligned with the steepest descent - or in the opposite direction of the gradient at that particular value. This leads to the following equation:\n",
    "\n",
    "$${\\displaystyle \\mathbf {x} ^{(k+1)}=\\mathbf {x} ^{(k)}-\\gamma \\nabla F(\\mathbf {x} ^{(k)})}$$\n",
    "\n",
    "Here, $\\gamma$ is called the 'learning rate' and corresponds to the size of the step taken in the steepest direction. Again, it is crucial to accurately compute the gradient $\\nabla F(\\mathbf {x} ^{(k)})$ in every iteration and this is where the automatic differentiation package comes in. For the moment, it is foreseen that the user would interact with the package as follows:\n",
    "\n",
    "```python\n",
    ">>> from AutoDiff.ForwardAd import Var\n",
    ">>> from AutoDiff import GradientDescent as GS\n",
    ">>> def F(x1, x2, x3):\n",
    "    return Var.sin(x1)**(Var.sqrt(x2)) + x3\n",
    ">>> x_0 = [0.1, 0, 2]\n",
    ">>> gamma = 0.1\n",
    ">>> argmin, min_F = GS(F, x_0, gamma)\n",
    "```\n",
    "\n",
    "Note that argmin stands for the array of values for $\\mathbf {x}$ that minimizes F and min_F corresponds of the actual value of the function F at that minimum. Also, it is important to know that teh gradient descent algorithm does not guarantee an absolute minimum, but through clever implementations it is able to find a local minimum that satisfies the criteria in practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vfMc8GfA08Qe"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Milestone2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
